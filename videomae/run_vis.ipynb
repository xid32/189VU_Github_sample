{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ,subprocess\n",
    "TestVideoNames = [\n",
    "    \"data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi\",\n",
    "    \"data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi\",\n",
    "    \"data/UCF-101/Archery/v_Archery_g01_c01.avi\",\n",
    "    \"data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi\",\n",
    "    \"data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi\",\n",
    "    \"data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi\",\n",
    "    \"data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi\",\n",
    "    \"data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi\",\n",
    "]\n",
    "MODEL_PATH = \"logs_modified_attention/checkpoint-1199.pth\"\n",
    "LOG_PATH = \"./results/VideoMAE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_name(video_path:str):\n",
    "    video_file = video_path.split(\"/\")[-1]\n",
    "    return video_file.split(\"_\")[1]\n",
    "def format_output_dir(video_name,mask_ratio):\n",
    "    return video_name+\"_\"+\"mask_ratio_\"+str(mask_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.2 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi ./results/VideoMAE/ApplyEyeMakeup_mask_ratio_0.2 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi', save_path='./results/VideoMAE/ApplyEyeMakeup_mask_ratio_0.2', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.2, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.4 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi ./results/VideoMAE/ApplyEyeMakeup_mask_ratio_0.4 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi', save_path='./results/VideoMAE/ApplyEyeMakeup_mask_ratio_0.4', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.4, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.6 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi ./results/VideoMAE/ApplyEyeMakeup_mask_ratio_0.6 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi', save_path='./results/VideoMAE/ApplyEyeMakeup_mask_ratio_0.6', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.6, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.8 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi ./results/VideoMAE/ApplyEyeMakeup_mask_ratio_0.8 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi', save_path='./results/VideoMAE/ApplyEyeMakeup_mask_ratio_0.8', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.8, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi ./results/VideoMAE/ApplyEyeMakeup_mask_ratio_0.9 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi', save_path='./results/VideoMAE/ApplyEyeMakeup_mask_ratio_0.9', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.2 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi ./results/VideoMAE/ApplyLipstick_mask_ratio_0.2 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi', save_path='./results/VideoMAE/ApplyLipstick_mask_ratio_0.2', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.2, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.4 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi ./results/VideoMAE/ApplyLipstick_mask_ratio_0.4 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi', save_path='./results/VideoMAE/ApplyLipstick_mask_ratio_0.4', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.4, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.6 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi ./results/VideoMAE/ApplyLipstick_mask_ratio_0.6 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi', save_path='./results/VideoMAE/ApplyLipstick_mask_ratio_0.6', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.6, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.8 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi ./results/VideoMAE/ApplyLipstick_mask_ratio_0.8 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi', save_path='./results/VideoMAE/ApplyLipstick_mask_ratio_0.8', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.8, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi ./results/VideoMAE/ApplyLipstick_mask_ratio_0.9 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi', save_path='./results/VideoMAE/ApplyLipstick_mask_ratio_0.9', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.2 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/Archery/v_Archery_g01_c01.avi ./results/VideoMAE/Archery_mask_ratio_0.2 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/Archery/v_Archery_g01_c01.avi', save_path='./results/VideoMAE/Archery_mask_ratio_0.2', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.2, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.4 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/Archery/v_Archery_g01_c01.avi ./results/VideoMAE/Archery_mask_ratio_0.4 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/Archery/v_Archery_g01_c01.avi', save_path='./results/VideoMAE/Archery_mask_ratio_0.4', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.4, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.6 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/Archery/v_Archery_g01_c01.avi ./results/VideoMAE/Archery_mask_ratio_0.6 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/Archery/v_Archery_g01_c01.avi', save_path='./results/VideoMAE/Archery_mask_ratio_0.6', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.6, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.8 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/Archery/v_Archery_g01_c01.avi ./results/VideoMAE/Archery_mask_ratio_0.8 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/Archery/v_Archery_g01_c01.avi', save_path='./results/VideoMAE/Archery_mask_ratio_0.8', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.8, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/Archery/v_Archery_g01_c01.avi ./results/VideoMAE/Archery_mask_ratio_0.9 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/Archery/v_Archery_g01_c01.avi', save_path='./results/VideoMAE/Archery_mask_ratio_0.9', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.2 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi ./results/VideoMAE/BabyCrawling_mask_ratio_0.2 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi', save_path='./results/VideoMAE/BabyCrawling_mask_ratio_0.2', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.2, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.4 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi ./results/VideoMAE/BabyCrawling_mask_ratio_0.4 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi', save_path='./results/VideoMAE/BabyCrawling_mask_ratio_0.4', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.4, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.6 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi ./results/VideoMAE/BabyCrawling_mask_ratio_0.6 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi', save_path='./results/VideoMAE/BabyCrawling_mask_ratio_0.6', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.6, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.8 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi ./results/VideoMAE/BabyCrawling_mask_ratio_0.8 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi', save_path='./results/VideoMAE/BabyCrawling_mask_ratio_0.8', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.8, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi ./results/VideoMAE/BabyCrawling_mask_ratio_0.9 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi', save_path='./results/VideoMAE/BabyCrawling_mask_ratio_0.9', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.2 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi ./results/VideoMAE/BalanceBeam_mask_ratio_0.2 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi', save_path='./results/VideoMAE/BalanceBeam_mask_ratio_0.2', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.2, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.4 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi ./results/VideoMAE/BalanceBeam_mask_ratio_0.4 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi', save_path='./results/VideoMAE/BalanceBeam_mask_ratio_0.4', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.4, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.6 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi ./results/VideoMAE/BalanceBeam_mask_ratio_0.6 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi', save_path='./results/VideoMAE/BalanceBeam_mask_ratio_0.6', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.6, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.8 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi ./results/VideoMAE/BalanceBeam_mask_ratio_0.8 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi', save_path='./results/VideoMAE/BalanceBeam_mask_ratio_0.8', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.8, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi ./results/VideoMAE/BalanceBeam_mask_ratio_0.9 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi', save_path='./results/VideoMAE/BalanceBeam_mask_ratio_0.9', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.2 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi ./results/VideoMAE/BandMarching_mask_ratio_0.2 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi', save_path='./results/VideoMAE/BandMarching_mask_ratio_0.2', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.2, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.4 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi ./results/VideoMAE/BandMarching_mask_ratio_0.4 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi', save_path='./results/VideoMAE/BandMarching_mask_ratio_0.4', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.4, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.6 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi ./results/VideoMAE/BandMarching_mask_ratio_0.6 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi', save_path='./results/VideoMAE/BandMarching_mask_ratio_0.6', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.6, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.8 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi ./results/VideoMAE/BandMarching_mask_ratio_0.8 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi', save_path='./results/VideoMAE/BandMarching_mask_ratio_0.8', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.8, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi ./results/VideoMAE/BandMarching_mask_ratio_0.9 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi', save_path='./results/VideoMAE/BandMarching_mask_ratio_0.9', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.2 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi ./results/VideoMAE/BasketballDunk_mask_ratio_0.2 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi', save_path='./results/VideoMAE/BasketballDunk_mask_ratio_0.2', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.2, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/code/VideoMAE/run_videomae_vis.py\", line 209, in <module>\n",
      "    main(opts)\n",
      "  File \"/home/ubuntu/code/VideoMAE/run_videomae_vis.py\", line 134, in main\n",
      "    video_data = vr.get_batch(frame_id_list).asnumpy()\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 174, in get_batch\n",
      "    indices = _nd.array(self._validate_indices(indices))\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 132, in _validate_indices\n",
      "    raise IndexError('Out of bound indices: {}'.format(indices[indices >= self._num_frame]))\n",
      "IndexError: Out of bound indices: [62 64 66 68 70 72 74 76 78 80 82 84 86 88 90]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.4 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi ./results/VideoMAE/BasketballDunk_mask_ratio_0.4 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi', save_path='./results/VideoMAE/BasketballDunk_mask_ratio_0.4', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.4, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/code/VideoMAE/run_videomae_vis.py\", line 209, in <module>\n",
      "    main(opts)\n",
      "  File \"/home/ubuntu/code/VideoMAE/run_videomae_vis.py\", line 134, in main\n",
      "    video_data = vr.get_batch(frame_id_list).asnumpy()\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 174, in get_batch\n",
      "    indices = _nd.array(self._validate_indices(indices))\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 132, in _validate_indices\n",
      "    raise IndexError('Out of bound indices: {}'.format(indices[indices >= self._num_frame]))\n",
      "IndexError: Out of bound indices: [62 64 66 68 70 72 74 76 78 80 82 84 86 88 90]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.6 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi ./results/VideoMAE/BasketballDunk_mask_ratio_0.6 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi', save_path='./results/VideoMAE/BasketballDunk_mask_ratio_0.6', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.6, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/code/VideoMAE/run_videomae_vis.py\", line 209, in <module>\n",
      "    main(opts)\n",
      "  File \"/home/ubuntu/code/VideoMAE/run_videomae_vis.py\", line 134, in main\n",
      "    video_data = vr.get_batch(frame_id_list).asnumpy()\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 174, in get_batch\n",
      "    indices = _nd.array(self._validate_indices(indices))\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 132, in _validate_indices\n",
      "    raise IndexError('Out of bound indices: {}'.format(indices[indices >= self._num_frame]))\n",
      "IndexError: Out of bound indices: [62 64 66 68 70 72 74 76 78 80 82 84 86 88 90]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.8 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi ./results/VideoMAE/BasketballDunk_mask_ratio_0.8 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi', save_path='./results/VideoMAE/BasketballDunk_mask_ratio_0.8', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.8, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/code/VideoMAE/run_videomae_vis.py\", line 209, in <module>\n",
      "    main(opts)\n",
      "  File \"/home/ubuntu/code/VideoMAE/run_videomae_vis.py\", line 134, in main\n",
      "    video_data = vr.get_batch(frame_id_list).asnumpy()\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 174, in get_batch\n",
      "    indices = _nd.array(self._validate_indices(indices))\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 132, in _validate_indices\n",
      "    raise IndexError('Out of bound indices: {}'.format(indices[indices >= self._num_frame]))\n",
      "IndexError: Out of bound indices: [62 64 66 68 70 72 74 76 78 80 82 84 86 88 90]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi ./results/VideoMAE/BasketballDunk_mask_ratio_0.9 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi', save_path='./results/VideoMAE/BasketballDunk_mask_ratio_0.9', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/code/VideoMAE/run_videomae_vis.py\", line 209, in <module>\n",
      "    main(opts)\n",
      "  File \"/home/ubuntu/code/VideoMAE/run_videomae_vis.py\", line 134, in main\n",
      "    video_data = vr.get_batch(frame_id_list).asnumpy()\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 174, in get_batch\n",
      "    indices = _nd.array(self._validate_indices(indices))\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 132, in _validate_indices\n",
      "    raise IndexError('Out of bound indices: {}'.format(indices[indices >= self._num_frame]))\n",
      "IndexError: Out of bound indices: [62 64 66 68 70 72 74 76 78 80 82 84 86 88 90]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.2 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi ./results/VideoMAE/BlowDryHair_mask_ratio_0.2 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi', save_path='./results/VideoMAE/BlowDryHair_mask_ratio_0.2', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.2, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.4 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi ./results/VideoMAE/BlowDryHair_mask_ratio_0.4 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi', save_path='./results/VideoMAE/BlowDryHair_mask_ratio_0.4', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.4, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.6 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi ./results/VideoMAE/BlowDryHair_mask_ratio_0.6 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi', save_path='./results/VideoMAE/BlowDryHair_mask_ratio_0.6', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.6, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.8 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi ./results/VideoMAE/BlowDryHair_mask_ratio_0.8 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi', save_path='./results/VideoMAE/BlowDryHair_mask_ratio_0.8', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.8, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi ./results/VideoMAE/BlowDryHair_mask_ratio_0.9 logs_modified_attention/checkpoint-1199.pth\n",
      "Namespace(img_path='data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi', save_path='./results/VideoMAE/BlowDryHair_mask_ratio_0.9', model_path='logs_modified_attention/checkpoint-1199.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:1', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224]) torch.Size([1, 1, 2048, 13])\n"
     ]
    }
   ],
   "source": [
    "for v in TestVideoNames:\n",
    "    for mr in [0.2,0.4,0.6,0.8,0.9]:\n",
    "        vn = get_video_name(v)\n",
    "        output_dir = os.path.join(LOG_PATH,format_output_dir(vn,mr))\n",
    "        command = f\"/home/ubuntu/miniconda3/envs/pytorch/bin/python run_videomae_vis.py --mask_ratio {mr} --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 {v} {output_dir} {MODEL_PATH}\"\n",
    "        print(command)\n",
    "        subprocess.run(command.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.5 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi ./results/VideoMAE_Original/ApplyEyeMakeup_mask_ratio_0.5 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi', save_path='./results/VideoMAE_Original/ApplyEyeMakeup_mask_ratio_0.5', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.5, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.75 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi ./results/VideoMAE_Original/ApplyEyeMakeup_mask_ratio_0.75 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi', save_path='./results/VideoMAE_Original/ApplyEyeMakeup_mask_ratio_0.75', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.75, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi ./results/VideoMAE_Original/ApplyEyeMakeup_mask_ratio_0.9 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi', save_path='./results/VideoMAE_Original/ApplyEyeMakeup_mask_ratio_0.9', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.5 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi ./results/VideoMAE_Original/ApplyLipstick_mask_ratio_0.5 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi', save_path='./results/VideoMAE_Original/ApplyLipstick_mask_ratio_0.5', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.5, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.75 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi ./results/VideoMAE_Original/ApplyLipstick_mask_ratio_0.75 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi', save_path='./results/VideoMAE_Original/ApplyLipstick_mask_ratio_0.75', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.75, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi ./results/VideoMAE_Original/ApplyLipstick_mask_ratio_0.9 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/ApplyLipstick/v_ApplyLipstick_g01_c01.avi', save_path='./results/VideoMAE_Original/ApplyLipstick_mask_ratio_0.9', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.5 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/Archery/v_Archery_g01_c01.avi ./results/VideoMAE_Original/Archery_mask_ratio_0.5 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/Archery/v_Archery_g01_c01.avi', save_path='./results/VideoMAE_Original/Archery_mask_ratio_0.5', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.5, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.75 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/Archery/v_Archery_g01_c01.avi ./results/VideoMAE_Original/Archery_mask_ratio_0.75 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/Archery/v_Archery_g01_c01.avi', save_path='./results/VideoMAE_Original/Archery_mask_ratio_0.75', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.75, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/Archery/v_Archery_g01_c01.avi ./results/VideoMAE_Original/Archery_mask_ratio_0.9 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/Archery/v_Archery_g01_c01.avi', save_path='./results/VideoMAE_Original/Archery_mask_ratio_0.9', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.5 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi ./results/VideoMAE_Original/BabyCrawling_mask_ratio_0.5 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi', save_path='./results/VideoMAE_Original/BabyCrawling_mask_ratio_0.5', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.5, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.75 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi ./results/VideoMAE_Original/BabyCrawling_mask_ratio_0.75 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi', save_path='./results/VideoMAE_Original/BabyCrawling_mask_ratio_0.75', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.75, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi ./results/VideoMAE_Original/BabyCrawling_mask_ratio_0.9 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/BabyCrawling/v_BabyCrawling_g01_c01.avi', save_path='./results/VideoMAE_Original/BabyCrawling_mask_ratio_0.9', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.5 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi ./results/VideoMAE_Original/BalanceBeam_mask_ratio_0.5 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi', save_path='./results/VideoMAE_Original/BalanceBeam_mask_ratio_0.5', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.5, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.75 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi ./results/VideoMAE_Original/BalanceBeam_mask_ratio_0.75 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi', save_path='./results/VideoMAE_Original/BalanceBeam_mask_ratio_0.75', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.75, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi ./results/VideoMAE_Original/BalanceBeam_mask_ratio_0.9 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/BalanceBeam/v_BalanceBeam_g01_c01.avi', save_path='./results/VideoMAE_Original/BalanceBeam_mask_ratio_0.9', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.5 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi ./results/VideoMAE_Original/BandMarching_mask_ratio_0.5 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi', save_path='./results/VideoMAE_Original/BandMarching_mask_ratio_0.5', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.5, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.75 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi ./results/VideoMAE_Original/BandMarching_mask_ratio_0.75 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi', save_path='./results/VideoMAE_Original/BandMarching_mask_ratio_0.75', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.75, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi ./results/VideoMAE_Original/BandMarching_mask_ratio_0.9 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/BandMarching/v_BandMarching_g01_c01.avi', save_path='./results/VideoMAE_Original/BandMarching_mask_ratio_0.9', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.5 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi ./results/VideoMAE_Original/BasketballDunk_mask_ratio_0.5 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi', save_path='./results/VideoMAE_Original/BasketballDunk_mask_ratio_0.5', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.5, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/code/VideoMAE/VideoMAE/run_videomae_vis.py\", line 185, in <module>\n",
      "    main(opts)\n",
      "  File \"/home/ubuntu/code/VideoMAE/VideoMAE/run_videomae_vis.py\", line 126, in main\n",
      "    video_data = vr.get_batch(frame_id_list).asnumpy()\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 174, in get_batch\n",
      "    indices = _nd.array(self._validate_indices(indices))\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 132, in _validate_indices\n",
      "    raise IndexError('Out of bound indices: {}'.format(indices[indices >= self._num_frame]))\n",
      "IndexError: Out of bound indices: [62 64 66 68 70 72 74 76 78 80 82 84 86 88 90]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.75 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi ./results/VideoMAE_Original/BasketballDunk_mask_ratio_0.75 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi', save_path='./results/VideoMAE_Original/BasketballDunk_mask_ratio_0.75', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.75, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/code/VideoMAE/VideoMAE/run_videomae_vis.py\", line 185, in <module>\n",
      "    main(opts)\n",
      "  File \"/home/ubuntu/code/VideoMAE/VideoMAE/run_videomae_vis.py\", line 126, in main\n",
      "    video_data = vr.get_batch(frame_id_list).asnumpy()\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 174, in get_batch\n",
      "    indices = _nd.array(self._validate_indices(indices))\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 132, in _validate_indices\n",
      "    raise IndexError('Out of bound indices: {}'.format(indices[indices >= self._num_frame]))\n",
      "IndexError: Out of bound indices: [62 64 66 68 70 72 74 76 78 80 82 84 86 88 90]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi ./results/VideoMAE_Original/BasketballDunk_mask_ratio_0.9 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/BasketballDunk/v_BasketballDunk_g01_c01.avi', save_path='./results/VideoMAE_Original/BasketballDunk_mask_ratio_0.9', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/code/VideoMAE/VideoMAE/run_videomae_vis.py\", line 185, in <module>\n",
      "    main(opts)\n",
      "  File \"/home/ubuntu/code/VideoMAE/VideoMAE/run_videomae_vis.py\", line 126, in main\n",
      "    video_data = vr.get_batch(frame_id_list).asnumpy()\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 174, in get_batch\n",
      "    indices = _nd.array(self._validate_indices(indices))\n",
      "  File \"/home/ubuntu/miniconda3/envs/pytorch/lib/python3.9/site-packages/decord/video_reader.py\", line 132, in _validate_indices\n",
      "    raise IndexError('Out of bound indices: {}'.format(indices[indices >= self._num_frame]))\n",
      "IndexError: Out of bound indices: [62 64 66 68 70 72 74 76 78 80 82 84 86 88 90]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.5 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi ./results/VideoMAE_Original/BlowDryHair_mask_ratio_0.5 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi', save_path='./results/VideoMAE_Original/BlowDryHair_mask_ratio_0.5', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.5, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.75 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi ./results/VideoMAE_Original/BlowDryHair_mask_ratio_0.75 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi', save_path='./results/VideoMAE_Original/BlowDryHair_mask_ratio_0.75', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.75, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n",
      "/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio 0.9 --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi ./results/VideoMAE_Original/BlowDryHair_mask_ratio_0.9 logs_original/checkpoint-499.pth\n",
      "Namespace(img_path='data/UCF-101/BlowDryHair/v_BlowDryHair_g01_c01.avi', save_path='./results/VideoMAE_Original/BlowDryHair_mask_ratio_0.9', model_path='logs_original/checkpoint-499.pth', mask_type='tube', num_frames=16, sampling_rate=4, decoder_depth=4, input_size=224, device='cuda:0', imagenet_default_mean_and_std=True, mask_ratio=0.9, model='pretrain_videomae_small_patch16_224', drop_path=0.0)\n",
      "Creating model: pretrain_videomae_small_patch16_224\n",
      "Patch size = (16, 16)\n",
      "(16, 240, 320, 3)\n",
      "torch.Size([1, 3, 16, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"logs_original/checkpoint-499.pth\"\n",
    "LOG_PATH = \"./results/VideoMAE_Original\"\n",
    "for v in TestVideoNames:\n",
    "    for mr in [0.5,0.75,0.9]:\n",
    "        vn = get_video_name(v)\n",
    "        output_dir = os.path.join(LOG_PATH,format_output_dir(vn,mr))\n",
    "        command = f\"/home/ubuntu/miniconda3/envs/pytorch/bin/python VideoMAE/run_videomae_vis.py --mask_ratio {mr} --mask_type tube --decoder_depth 4 --model pretrain_videomae_small_patch16_224 {v} {output_dir} {MODEL_PATH}\"\n",
    "        print(command)\n",
    "        subprocess.run(command.split(\" \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
